 
input {
  file {
    path => "/var/log/app/*.log"  # Adjust this to your actual log file path
    start_position => "beginning"
    sincedb_path => "/dev/null"  # Avoid state tracking for dev
  }
}

filter {
  grok {
    match => { "message" => "%{TIMESTAMP_ISO8601:timestamp} \[%{LOGLEVEL:loglevel}\] %{GREEDYDATA:msg}" }
  }
  date {
    match => ["timestamp", "ISO8601"]
  }
}

output {
  elasticsearch {
    hosts => ["http://elasticsearch:9200"]
    index => "summiva-logs-%{+YYYY.MM.dd}"
  }

  stdout { codec => rubydebug }
}
    
##   The configuration file is pretty straightforward. It reads log files from the  /var/log/app  directory, parses the log lines using the  grok  filter, and sends the parsed logs to Elasticsearch. 
##   The  grok  filter is used to parse the log lines. It uses a pattern to match the log lines and extract the timestamp, log level, and message. The  date  filter is used to parse the timestamp extracted by the  grok  filter. 
##   The  output  section sends the parsed logs to Elasticsearch. The  elasticsearch  output plugin is used to send the logs to Elasticsearch. The  stdout  output plugin is used to print the logs to the console. 
##   Step 4: Start the ELK Stack 
##   Now that we have the configuration files ready, we can start the ELK stack using Docker Compose. 
##   Run the following command to start the ELK stack: